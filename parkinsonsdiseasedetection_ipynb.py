# -*- coding: utf-8 -*-
"""parkinsonsdiseasedetection ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/116X0apwrSwhRvTjiI4v8ILV2rN20n3n3
"""

!pip install lux

!pip install imblearn

!pip install xgboost

import numpy as np
import pandas as pd
import os,sys
import lux 
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#load dataset

df = pd.read_csv('parkinsons.csv')
df.head(n=10)



df.head(n=10)

df.shape

#preprocessing:check null values in dataset
df.isnull().sum()

df.dtypes

#finding unique values in the colums
for i in df.columns:
  print("**********************************************",i,"***********************************************************")
  print()
  print(set(df[i].tolist()))
  print()

temp=df["status"].value_counts()
temp

#checklabel imbalance
import matplotlib.pyplot as plt
import seaborn as sns

temp=df["status"].value_counts()
temp_df=pd.DataFrame({'status':temp.index,'values':temp.values})
print(sns.barplot(x='status',y="values",data=temp_df))

temp_df

sns.pairplot(df)

sns.distplot(df["PPE"])

def distplot(col):
  sns.distplot(df[col])
  plt.show()
for i in list(df.columns)[1:]:
  distplot(i)

#find the distribution of  data
def boxplots(col):
  sns.boxplot(df[col])
  plt.show()
for i in list(df.select_dtypes(exclude=["object"]).columns)[1:]:
  boxplots(i)

# findingcorelation
plt.figure(figsize=(20,20))
corr=df.corr()
sns.heatmap(corr,annot=True)

#seperateindependent and dependent variablesand drop ic column
x=df.drop(["status","name"],axis=1)
y=df["status"]

#lets detect the label imbalance
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
print(Counter(y))

#lets baance the labels
ros=RandomOverSampler()
x_ros,y_ros=ros.fit_resample(x,y)
print(Counter(y_ros))

#initialize the MinMaxScaler and scale the features to between-1 and 1 to normalize them.
#the MinMaxScalar transform features by scaling them a given range
#the fit_transform() method fits tothe data and then transform it. We dont need to scale the labels.
#scale the features to between -1 to 1
scaler=MinMaxScaler((-1,1))
x=scaler.fit_transform(x_ros)
y=y_ros

#the most import use of pca is to reprent a multivariate data table as a smaller set of variables(summary indices)
#in order to  observe trends,jumps,clusters and outliers.
#this overview may uncover the relationships between observations and variables and among the variables

#applying feature enginnering
#applying pca
#the code below .95 for no of componets parameter.
#it meansthat scikitlearn choose the minimum no of pricipal components such that 95% of the varianceis retained .
from sklearn.decomposition import PCA
pca=PCA(.95)
X_PCA=pca.fit_transform(x)
print(x.shape)
print(X_PCA.shape)
#thus we need 8 columns to keep 95% of the variance

#now wesplit the data into traning and testing sets keeping 20% of the data for testing.
#Split the data set
x_train,x_test,y_train,y_test=train_test_split(X_PCA,y,test_size=0.2,random_state=7)

#applying the algorithm
from sklearn.metrics import confusion_matrix,accuracy_score,f1_score,precision_score,recall_score
list_met=[]
list_accuracy=[]
#applying all thealgoritms 
#apply logistic regression
from sklearn.linear_model import LogisticRegression
classifier=LogisticRegression(C=0.4,max_iter=1000,solver='liblinear')
lr=classifier.fit(x_train,y_train)
#prediction
y_pred=classifier.predict(x_test)
#accuracy
accuracy_LR=accuracy_score(y_test,y_pred)

#apply decission tree

from sklearn.tree import DecisionTreeClassifier
classifier2=DecisionTreeClassifier(random_state=14)
dt=classifier2.fit(x_train,y_train)
#prediction
y_pred2=classifier2.predict(x_test)
#accuracy
accuracy_DT=accuracy_score(y_test,y_pred2)



#APPLY RANDOM FOREST CRITERIA-INFORMATION GAIN
from sklearn.ensemble import RandomForestClassifier
classifier3=RandomForestClassifier(random_state=14)
rfi=classifier3.fit(x_train,y_train)
#prediction
y_pred3=classifier3.predict(x_test)
#accuracy
accuracy_RFI=accuracy_score(y_test,y_pred3)


#apply random forest criteria entropy


from sklearn.ensemble import RandomForestClassifier
classifier4=RandomForestClassifier(criterion='entropy')
rfe=classifier4.fit(x_train,y_train)
#prediction
y_pred4=classifier4.predict(x_test)
#accuracy
accuracy_RFE=accuracy_score(y_test,y_pred4)



#APPLY SVM
from sklearn.svm import SVC
model_svm=SVC(cache_size=100)
svm=model_svm.fit(x_train,y_train)
#prediction
y_pred5=model_svm.predict(x_test)
#accuracy
accuracy_svc=accuracy_score(y_test,y_pred5)



#apply knn
from sklearn.neighbors import KNeighborsClassifier
model_knn3=KNeighborsClassifier(n_neighbors=3)
knn=model_knn3.fit(x_train,y_train)
#prediction test set n=3
pred_knn3=model_knn3.predict(x_test)
#accuracy
accuracy_SVM=accuracy_score(y_test,y_pred5)


#APPLYING bernollinaive bayes classifier

from sklearn.naive_bayes import BernoulliNB
model= BernoulliNB()
bnb=model.fit(x_train,y_train)
#prediction test set n=3
pred_bnb=model.predict(x_test)
#accuracy
accuracy_BNB=accuracy_score(y_test,pred_bnb)


#APPLYING Gaussian naives bayes classifier

from sklearn.naive_bayes import GaussianNB
model= GaussianNB()
gnb=model.fit(x_train,y_train)
#prediction test set n=3
pred_gnb=gnb.predict(x_test)
#accuracy
accuracy_GNB=accuracy_score(y_test,pred_gnb)


#combining all the above using voting classifier

from sklearn.ensemble import VotingClassifier
evc=VotingClassifier(estimators=[('lr',lr),('rfi',rfi),('rfe',rfe),('DT',dt),('svm',svm),('knn',knn),('gnb',gnb),('bnb',bnb)],voting='hard',flatten_transform=True)
model_evc=evc.fit(x_train,y_train)
#prediction
pred_evc=evc.predict(x_test)
#accuracy
accuracy_evc=accuracy_score(y_test,pred_gnb)


list1=['Logistic Regression','Decision Tree',' Random Forest(information gain)','RandomForest(Entropy)','SVM','KNN','gnb','bnb','voting']
list2=[accuracy_LR,accuracy_DT,accuracy_RFI,accuracy_RFE,accuracy_svc,accuracy_SVM,accuracy_GNB,accuracy_BNB,accuracy_evc]
list3=[classifier,classifier2,classifier3,classifier4,model_svm,model_knn3,gnb,model]

df_Accuracy=pd.DataFrame({'Method used':list1,'Accuracy':list2})
print(df_Accuracy)
chart=sns.barplot(x="Method used",y="Accuracy",data=df_Accuracy)
chart.set_xticklabels(chart.get_xticklabels(),rotation=90)
print(chart)

#initialize an XGBClassifier and train the model
#this classsifies using extreme gradient boosting algorithms for mordern data science problem
#it falls under the category ofensemble learning in ML
#where er train and predict using many models to produce one superior output
#train the model

from xgboost import XGBClassifier
model_xg=XGBClassifier()
model_xg.fit(x_train,y_train)

#finally generate y_pred(prediced values for x testandcalculate the accuracy of the model)
#PRINT IT OUT 
#CALCULATE THE ACCURACY
y_pred=model_xg.predict(x_test)
print(accuracy_score(y_test,y_pred)*100)

from sklearn.metrics import confusion_matrix
cm=confusion_matrix(y_test,model_xg.predict(x_test))

from sklearn.metrics import f1_score
f1_score(y_test,model_xg.predict(x_test),average='binary')

from sklearn.metrics import roc_curve,auc,confusion_matrix,classification_report,accuracy_score
print(classification_report(y_test,model_xg.predict(x_test)))
print('Confusion Matrix:')
print(cm)

for i in list3:
  print("******************************************************",i,"*******************************")
  print(classification_report(y_test,i.predict(x_test)))
  print('Confusion Matrix:')
  print(cm)
  print()

#visualizing performance with roc
from sklearn.metrics import roc_curve,auc,confusion_matrix,classification_report,accuracy_score
def plot_roc(model,x_test,y_test):
  #calculate the fpr and tpr of all threshold of the classifications
  probabilities=model.predict_proba(np.array(x_test))
  predictions=probabilities
  fpr,tpr,threshold=roc_curve(y_test,predictions[:,1])
  roc_auc=auc(fpr,tpr)

  plt.title('reciever operating characterstics')
  plt.plot(fpr,tpr,'b',label='AUC= %0.2f' % roc_auc)
  plt.legend(loc='lower right')
#   plt.plot([0,1][0,1],'r--')
  plt.plot(0,1,'r--')  
#   plt.xlim([0,1])
  plt.xlim(0,1)
#   plt.ylim([0,1])
  plt.ylim(0,1)
  plt.ylabel('True Positive Rate')
  plt.xlabel('False Positive Rate')
  plt.show()

plot_roc(model_xg,x_test,y_test)

for i in range(0,len(list3)):
  try:
    print()
    print("_________________________ROC FOR",list1[i],"+PCA________________---")
    plot_roc(list3[i],x_test,np.array(y_test))
    print()
  except:
    print("roc not valid")





